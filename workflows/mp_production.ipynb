{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from shutil import copyfile\n",
    "import os, subprocess, pipes, re, json, glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from fireworks.fw_config import SORT_FWS\n",
    "from fireworks.core.launchpad import LaunchPad\n",
    "from fireworks.core.fworker import FWorker\n",
    "from boltons.tbutils import ParsedException\n",
    "REMOTE_GARDEN = '/global/projecta/projectdirs/matgen/garden/'\n",
    "LOCAL_GARDEN = '/Users/patrick/Downloads/mp_prod_garden/' # ADJUST ME\n",
    "LAUNCHDIRS = os.path.join(LOCAL_GARDEN, 'launchdirs.txt')\n",
    "# don't forget to set up ssh tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lpad = LaunchPad.from_file('my_launchpad.yaml')\n",
    "fworker = FWorker.from_file('my_fworker.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_categories(cats):\n",
    "    c = Counter(dict((k, len(v)) for k, v in cats.items()))\n",
    "    top10 = c.most_common(10)\n",
    "    total = 0\n",
    "    for k,v in top10:\n",
    "        print(v, '\\t', cats[k], '\\t', k)\n",
    "        total += v\n",
    "    print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states = [\"RUNNING\", \"WAITING\", \"FIZZLED\", \"READY\", \"COMPLETED\", \"RESERVED\", \"ARCHIVED\", \"DEFUSED\", \"PAUSED\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fizzled workflows and according list of fireworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fw_ids for user-submitted workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_query = {\"spec.task_type\": \"Add to SNL database\", \"spec.snl.about.remarks\": \"MP user submission\"}\n",
    "# user_query = {\"spec.task_type\": \"Add to SNL database\", \"spec.snl.about.remarks\": \"new ICSD batch\"}\n",
    "# user_query = {\"spec.task_type\": \"Add to SNL database\", \"spec.snl.about.remarks\": \"Pauling file\"}\n",
    "fw_ids_user = lpad.fireworks.find(user_query, {'fw_id': 1, '_id': 0}).distinct('fw_id')\n",
    "print(len(fw_ids_user), 'user-submitted workflows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prioritized user-submitted \"Add to SNL\" tasks to get duplicate checking done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "priority_user_query = {\n",
    "    \"spec.task_type\": \"Add to SNL database\", \"spec.snl.about.remarks\": {\n",
    "        \"$in\": [\"MP user submission\"],\n",
    "        \"$nin\": [\"new ICSD batch\", \"Pauling file\", \"Heusler ABC2 phases\"]\n",
    "    }\n",
    "}\n",
    "priority_user_fws = {}\n",
    "for state in states:\n",
    "    if state == 'COMPLETED' or state == 'ARCHIVED':\n",
    "        continue\n",
    "    state_query = {'state': state}\n",
    "    state_query.update(priority_user_query)\n",
    "    priority_user_fws[state] = lpad.fireworks.find(state_query)\n",
    "    nr_fws = priority_user_fws[state].count()\n",
    "    if nr_fws > 0:\n",
    "        #for d in priority_user_fws[state]:\n",
    "        #    print(d['fw_id'])\n",
    "        print('{} {} user-submitted Add-to-SNL tasks'.format(nr_fws, state))\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### percentage of workflows in each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 118151 = {Ti,Zr,Hf}-Zn-N piezoelectricity study -> ALL COMPLETED 2017-01-24\n",
    "# 114781 = Kitchaev Workflows\n",
    "# 115780 = Heusler ABC2 phases\n",
    "submission_group_id = 114781\n",
    "query = {'nodes': {'$in': fw_ids_user}}\n",
    "if user_query[\"spec.snl.about.remarks\"] == \"MP user submission\":\n",
    "    print('FYI: only looking at workflows with submission_group_id', submission_group_id)\n",
    "    query.update({'metadata.submission_group_id': submission_group_id})\n",
    "wflows = {}\n",
    "total_wflows = float(lpad.workflows.find(query).count())\n",
    "wflows_projection = {'fw_states': 1, 'parent_links': 1, 'links': 1, 'nodes': 1, '_id': 0, 'state': 1}\n",
    "for state in states:\n",
    "    state_query = {'state': state}\n",
    "    state_query.update(query)\n",
    "    wflows[state] = list(lpad.workflows.find(state_query, wflows_projection))\n",
    "    nr_wflows = len(wflows[state])\n",
    "    if nr_wflows > 0:\n",
    "        wflows_fraction =  nr_wflows / total_wflows\n",
    "        print('{} {} workflows ({:.1f}%)'.format(nr_wflows, state, wflows_fraction*100.))\n",
    "print(int(total_wflows), 'workflows in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of first fizzled fw_id in each workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_root_node(wflow):\n",
    "    # wflow['nodes'][0] is not necessarily the root node!\n",
    "    parent_links_keys = wflow['parent_links'].keys()\n",
    "    for node in wflow['nodes']:\n",
    "        if str(node) in parent_links_keys:\n",
    "            continue\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = 'FIZZLED' # workflow state\n",
    "rerun_fws = []\n",
    "fw_ids_state = {}\n",
    "for wflow in wflows[state]:\n",
    "    root_fw_id = find_root_node(wflow)\n",
    "    # decend links until fizzled firework found\n",
    "    fw_id = root_fw_id\n",
    "    check_states = [state] if state != 'RUNNING' else ['READY', 'RESERVED']\n",
    "    while 1:\n",
    "        current_state = wflow['fw_states'][str(fw_id)]\n",
    "        if current_state == 'RUNNING':\n",
    "            print(fw_id, 'is RUNNING -> probably need to do `lpad rerun_fws -i {}`'.format(fw_id))\n",
    "            break\n",
    "        if current_state in check_states:\n",
    "            task_type = lpad.fireworks.find_one({'fw_id': fw_id}, {'spec.task_type': 1})['spec']['task_type']\n",
    "            if task_type not in fw_ids_state:\n",
    "                fw_ids_state[task_type] = [int(fw_id)]\n",
    "            else:\n",
    "                fw_ids_state[task_type].append(int(fw_id))\n",
    "            alt_state = lpad.fireworks.find_one({'fw_id': fw_id}, {'state': 1, '_id': 0})['state']\n",
    "            if alt_state == 'RESERVED':\n",
    "                rerun_fws.append(str(fw_id))\n",
    "            break\n",
    "        # if multiple children use non-waiting fw\n",
    "        children = wflow['links'][str(fw_id)]\n",
    "        for child in children:\n",
    "            if wflow['fw_states'][str(child)] != 'WAITING':\n",
    "                fw_id = child\n",
    "if rerun_fws:\n",
    "    print('lpad rerun_fws -i', ' '.join(rerun_fws))\n",
    "for k,v in fw_ids_state.items():\n",
    "    #if 'GGA' not in k: continue\n",
    "    print(k, v)\n",
    "    for fw_id in v:\n",
    "        launches = lpad.launches.find({'fw_id': fw_id}, {'launch_dir': 1})\n",
    "        for launch in launches:\n",
    "            if not 'oasis' in launch['launch_dir']:\n",
    "                print ('\\t', fw_id, launch['launch_dir'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of incomplete fireworks in RUNNING workflows for fworker query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw_ids_incomplete = {}\n",
    "for wflow in wflows['RUNNING']:\n",
    "    for fw_id, fw_state in wflow['fw_states'].items():\n",
    "        if fw_state != 'COMPLETED':\n",
    "            if fw_state not in fw_ids_incomplete:\n",
    "                fw_ids_incomplete[fw_state] = [int(fw_id)]\n",
    "            else:\n",
    "                fw_ids_incomplete[fw_state].append(int(fw_id))\n",
    "print(fw_ids_incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for d in lpad.workflows.find({'nodes': {'$in':[1370872,1566138,1566120,1566104,1566099,1567504,1567491,1563287,1652717]}}, {'_id': 0, 'nodes': 1}):\n",
    "    nodes += d['nodes']\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of first fireworks for fizzled workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {'fw_id': {'$in': [fw_id for fw_id in fw_ids_state.values()]}} # FIXME\n",
    "projection = {'fw_id': 1, 'launches': 1, '_id': 0}\n",
    "fws = list(lpad.fireworks.find(query, projection))\n",
    "assert(len(fws) == len(wflows[state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### launch directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fws_info = {}\n",
    "for fw in tqdm(fws):\n",
    "    launch_id = fw['launches'][-1]\n",
    "    launch = lpad.launches.find_one({'launch_id': launch_id}, {'launch_dir': 1, '_id': 0})\n",
    "    launch_dir = launch['launch_dir']\n",
    "    launch_dir_exists = False\n",
    "    for fw_id, fw_info in fws_info.items():\n",
    "        if launch_dir == fw_info['launch_dir']:\n",
    "            launch_dir_exists = True\n",
    "            break\n",
    "    if launch_dir_exists:\n",
    "        if 'duplicates' in fws_info[fw_id]:\n",
    "            fws_info[fw_id]['duplicates'].append(fw['fw_id'])\n",
    "        else:\n",
    "            fws_info[fw_id]['duplicates'] = [fw['fw_id']]\n",
    "        continue\n",
    "    fws_info[fw['fw_id']] = {'launch_dir': launch_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_duplicates = 0\n",
    "for fw_id, fw_info in fws_info.iteritems():\n",
    "    if 'duplicates' in fw_info:\n",
    "        nr_duplicates += len(fw_info['duplicates'])\n",
    "print(nr_duplicates, '/', len(fws), 'workflows have duplicate launch_dirs =>',\n",
    "      len(fws)-nr_duplicates, 'unique launch_dirs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write text file with list of remote files for rsync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to check existence of a list of files/directories; also generate list of existing output files in existing launch dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def launchdirs_exist(hostname):\n",
    "    # hostname from ~/.ssh/config\n",
    "    subprocess.call(['scp', LAUNCHDIRS, hostname+':~/'])\n",
    "    for f in glob.glob(os.path.join(LOCAL_GARDEN, '*.txt')):\n",
    "        os.remove(f)\n",
    "    subprocess.call([\n",
    "        'ssh', '-q', hostname,\n",
    "        'for i in `cat ~/launchdirs.txt`; do '\n",
    "        'if [ -d \"$i\" ]; then '\n",
    "        'echo $i >> ~/launchdirs_exist.txt; '\n",
    "        'compgen -G \"$i/*.out\" >> ~/launchdirs_exist_outfiles.txt; '\n",
    "        'compgen -G \"$i/*.error\" >> ~/launchdirs_exist_outfiles.txt; '\n",
    "        'else echo $i >> ~/launchdirs_not_exist.txt; fi; '\n",
    "        'done'\n",
    "    ])\n",
    "    subprocess.call(['scp', hostname+':~/launchdirs_*.txt', LOCAL_GARDEN])\n",
    "    subprocess.call(['ssh', '-q', hostname, 'rm ~/launchdirs*.txt'])\n",
    "    total_counts = 0\n",
    "    for fstr in glob.glob(os.path.join(LOCAL_GARDEN, 'launchdirs_*exist.txt')):\n",
    "        with open(fstr, 'r') as f:\n",
    "            counts = Counter(l.strip() for l in f)\n",
    "            total_counts += len(counts)\n",
    "            print(len(counts), '\\t', os.path.basename(fstr))\n",
    "    print('=', total_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### start by using launchdirs on NERSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(LAUNCHDIRS, 'w') as f:\n",
    "    for fw_id, fw_info in fws_info.iteritems():\n",
    "        print(fw_info['launch_dir'].strip(), file=f)\n",
    "launchdirs_exist('mendel-matcomp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try NERSC & XSEDE gardens as alternative path for non-existing launchdirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for XSEDE: rsync to Mendel from\n",
    "\n",
    "- /oasis/projects/nsf/csd436/phuck/garden\n",
    "- /oasis/scratch/comet/phuck/temp_project\n",
    "\n",
    "`rsync -avz block_* mendel:/global/projecta/projectdirs/matgen/garden/`  \n",
    "[could also do `try_garden` if direct ssh access to comet is enabled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dest_blocks(s):\n",
    "    a = s.strip().split('/block_')\n",
    "    if len(a) == 2:\n",
    "        return [a[0], 'block_'+a[1]]\n",
    "    a = s.strip().split('/launcher_')\n",
    "    return [a[0], 'launcher_'+a[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def try_garden(garden, hostname):\n",
    "    # hostname from ~/.ssh/config\n",
    "    launchdirs_exist_file = os.path.join(LOCAL_GARDEN, 'launchdirs_exist.txt')\n",
    "    if os.path.exists(launchdirs_exist_file):\n",
    "        copyfile(launchdirs_exist_file, LAUNCHDIRS)\n",
    "    with open(LAUNCHDIRS, 'a') as f1:\n",
    "        with open(os.path.join(LOCAL_GARDEN, 'launchdirs_not_exist.txt'), 'r') as f2:\n",
    "            for line in f2.readlines():\n",
    "                dest, block = get_dest_blocks(line)\n",
    "                remote_dir = os.path.join(garden, block)\n",
    "                print(remote_dir, file=f1)\n",
    "    launchdirs_exist(hostname)\n",
    "    #!head -1 {LOCAL_GARDEN}launchdirs_not_exist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try_garden(REMOTE_GARDEN, 'mendel-matcomp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rsync log output to local garden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dest_blocks = {}\n",
    "with open(os.path.join(LOCAL_GARDEN, 'launchdirs_exist_outfiles.txt'), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        dest, block_file = get_dest_blocks(line)\n",
    "        if dest in dest_blocks:\n",
    "            dest_blocks[dest].append(block_file)\n",
    "        else:\n",
    "            dest_blocks[dest] = [block_file]\n",
    "for k, v in dest_blocks.iteritems():\n",
    "    print('\\t', len(v), '\\t', k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dest, block_files in dest_blocks.items():\n",
    "    tmpfile = os.path.join(LOCAL_GARDEN, 'tmp.txt')\n",
    "    with open(tmpfile, 'w') as f:\n",
    "        for block_file in block_files:\n",
    "            print(block_file, file=f)\n",
    "    subprocess.call([\n",
    "        'rsync', '-av', '--files-from='+LOCAL_GARDEN+'/tmp.txt', 'mendel-matcomp:'+dest+'/', LOCAL_GARDEN\n",
    "    ])\n",
    "    os.remove(tmpfile)\n",
    "    print('done syncing', dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze log output of fizzled workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save actual remote and local output directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(LOCAL_GARDEN, 'launchdirs_exist.txt'), 'r') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        remote_dir = line.strip()\n",
    "        dest, block = get_dest_blocks(line)\n",
    "        block_found = False\n",
    "        for fw_id, fw_info in fws_info.iteritems():\n",
    "            if get_dest_blocks(fw_info['launch_dir'])[1] == block:\n",
    "                block_found = True\n",
    "                fw_info['remote_dir'] = remote_dir\n",
    "                fw_info['local_dir'] = os.path.join(LOCAL_GARDEN, block)\n",
    "                break\n",
    "        if not block_found:    \n",
    "            raise ValueError(block, 'not found')\n",
    "                \n",
    "# with open(os.path.join(LOCAL_GARDEN, 'launchdirs_not_exist.txt'), 'r') as f:\n",
    "#     for line in tqdm(f.readlines()):\n",
    "#         dest, block = get_dest_blocks(line)\n",
    "#         for fw_id, fw_info in fws_info.iteritems():\n",
    "#             if get_dest_blocks(fw_info['launch_dir'])[1] == block:\n",
    "#                 fw_info['remote_dir'] = None\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "launchdirs_not_exist_count = 0\n",
    "for fw_id, fw_info in fws_info.items():\n",
    "    launchdirs_not_exist_count += int('remote_dir' not in fw_info)\n",
    "print('check:', launchdirs_not_exist_count, 'launch_dirs not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scan for error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_path(extension, dirlist):\n",
    "    for fstr in dirlist:\n",
    "        fn, ext = os.path.splitext(os.path.basename(fstr))\n",
    "        if fn+ext == 'vasp.out':\n",
    "            continue\n",
    "        if ext == extension:\n",
    "            return fstr\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scan_errors_warnings(f):\n",
    "    for line in f.readlines():\n",
    "        line_lower = line.strip().lower()\n",
    "        if 'error:' in line_lower or 'warning:' in line_lower:\n",
    "            return line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for fw_id, fw_info in tqdm(fws_info.items()):\n",
    "    fw_info['errors'] = []\n",
    "    \n",
    "    if 'remote_dir' not in fw_info:\n",
    "        fw_info['errors'].append('remote_dir not found')\n",
    "        continue\n",
    "    local_dir = fw_info['local_dir']\n",
    "    if not os.path.exists(local_dir):\n",
    "        fw_info['errors'].append('local_dir not found')\n",
    "        continue\n",
    "    ls = glob.glob(os.path.join(local_dir, '*'))\n",
    "    if not ls:\n",
    "        fw_info['errors'].append('no files found in local_dir')\n",
    "        continue\n",
    "\n",
    "    error_file = get_file_path('.error', ls)\n",
    "    if error_file is not None:\n",
    "        # look for a traceback in *.error\n",
    "        with open(error_file, 'r') as f:\n",
    "            fcontent = f.read()\n",
    "            match = re.search('Traceback((.+\\n)+)Traceback', fcontent)\n",
    "            if not match:\n",
    "                match = re.search('Traceback((.+\\n)+)INFO', fcontent)\n",
    "                if not match:\n",
    "                    match = re.search('Traceback((.+\\n)+)$', fcontent)\n",
    "            if match:\n",
    "                fw_info['errors'].append('Traceback'+match.group(1))\n",
    "            else:\n",
    "                scan = scan_errors_warnings(f)\n",
    "                if scan:\n",
    "                    fw_info['errors'].append(scan)\n",
    "\n",
    "    # look into .out file\n",
    "    out_file = get_file_path('.out', ls)\n",
    "    with open(out_file, 'r') as f:\n",
    "        scan = scan_errors_warnings(f)\n",
    "        if scan:\n",
    "            fw_info['errors'].append(scan)\n",
    "\n",
    "    # look into vasp.out\n",
    "    vasp_out = os.path.join(local_dir, 'vasp.out')\n",
    "    if os.path.exists(vasp_out):\n",
    "        with open(vasp_out, 'r') as f:\n",
    "            vasp_out_tail = f.readlines()[-1].strip()\n",
    "            fw_info['errors'].append(' -- '.join(['vasp.out', vasp_out_tail]))\n",
    "\n",
    "# FIXME .out and .error for non-reservation mode on directory up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categorize errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_fw_to_category(fw_id, key, cats):\n",
    "    if key in cats:\n",
    "        cats[key].append(fw_id)\n",
    "    else:\n",
    "        cats[key] = [fw_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = {}\n",
    "for fw_id, fw_info in fws_info.iteritems():\n",
    "    if not fw_info['errors']:\n",
    "        add_fw_to_category(fw_id, 'no errors parsed', categories)\n",
    "        continue\n",
    "    for error in fw_info['errors']:\n",
    "        if 'remote_dir' in error or 'local_dir' in error:\n",
    "            add_fw_to_category(fw_id, error, categories)\n",
    "        elif error.startswith('Traceback'):       \n",
    "            exc = ParsedException.from_string(error)\n",
    "            msg = exc.exc_msg[:50]\n",
    "            match = re.search('errors reached: (.*)', msg)\n",
    "            if match:\n",
    "                msg = match.group(1)\n",
    "            key = ' -- '.join([exc.exc_type, msg])\n",
    "            lineno = exc.frames[-1]['lineno']\n",
    "            key = ' -- '.join([key, os.path.basename(exc.source_file) + '#' + lineno])\n",
    "            add_fw_to_category(fw_id, key, categories)\n",
    "        else:\n",
    "            match = re.search('{(.*)}', error) # matches dictionary\n",
    "            if match:\n",
    "                dstr = '{' + match.group(1) + '}'\n",
    "                dstr = dstr.replace(\"u'\", '\"').replace(\"'\", '\"')\n",
    "                dstr = re.sub('{\"handler\": (.*), \"errors\"', '{\"handler\": \"\\g<1>\", \"errors\"', dstr)\n",
    "                try:\n",
    "                    d = json.loads(dstr)\n",
    "                except:\n",
    "                    add_fw_to_category(fw_id, 'looks like dict but could not decode', categories)\n",
    "                else:\n",
    "                    if 'handler' in d and 'errors' in d:\n",
    "                        if '<' in d['handler']:\n",
    "                            match = re.search('custodian\\.vasp\\.handlers\\.(.*) object', d['handler'])\n",
    "                            if match:\n",
    "                                d['handler'] = match.group(1)\n",
    "                            else:\n",
    "                                raise ValueError('custodian.vasp.handlers not matched')\n",
    "                        add_fw_to_category(fw_id, d['handler'], categories)\n",
    "                    elif 'action' in d:\n",
    "                        add_fw_to_category(fw_id, 'action', categories)\n",
    "                    else:\n",
    "                        add_fw_to_category(fw_id, 'found dict but not handler or action error', categories)\n",
    "            else:\n",
    "                add_fw_to_category(fw_id, error, categories)\n",
    "        break # only look at first error\n",
    "print_categories(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fws_info[1564191]['remote_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lpad.fireworks.find_one({'fw_id': 1564191}, {'spec._priority': 1, 'state': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lpad.fireworks.find_one({'fw_id': 1285769}, {'spec._priority': 1, 'state': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lpad.fireworks.find_one({'fw_id': 1399045}, {'spec._priority': 1, 'state': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('mpcomplete_kitchaev.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_last_node(wflow):\n",
    "    for node in wflow['links'].keys():\n",
    "        if not wflow['links'][node]:\n",
    "            return node\n",
    "    raise ValueError('last node not found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "materials_prod_config_path = os.path.join(os.environ['DB_LOC'], 'materials_db_prod.yaml')\n",
    "materials_prod_config_file = open(materials_prod_config_path, 'r')\n",
    "config = yaml.load(materials_prod_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = MongoClient(config['host'], config['port'], j=False)\n",
    "db_jp = conn[config['db']]\n",
    "db_jp.authenticate(config['username'], config['password'])\n",
    "db_jp.materials.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cif, info in d.items():\n",
    "    submission_id = info['submission_id']\n",
    "    wflow = lpad.workflows.find_one({'metadata.submission_id': submission_id}, wflows_projection)\n",
    "    if wflow['state'] != 'COMPLETED':\n",
    "        continue\n",
    "    fw_id = find_root_node(wflow)\n",
    "    task_ids = [None]\n",
    "    while 1:\n",
    "        launch_id = lpad.fireworks.find_one({'fw_id': fw_id}, {'launches': 1, '_id': 0})['launches'][-1]\n",
    "        launch = lpad.launches.find_one(\n",
    "            {'launch_id': launch_id, 'action.stored_data.task_id': {'$exists': 1}},\n",
    "            {'action.stored_data.task_id': 1, '_id': 0}\n",
    "        )\n",
    "        if launch:\n",
    "            task_ids.append(launch['action']['stored_data']['task_id'])\n",
    "        children = wflow['links'][str(fw_id)]\n",
    "        if not children:\n",
    "            break\n",
    "        fw_id = children[-1]\n",
    "    mat = db_jp.materials.find_one({'task_ids': {'$in': task_ids}}, {'task_id': 1, 'task_ids': 1, '_id': 0})\n",
    "    info['fw_id'] = fw_id\n",
    "    info['mp_id'] = mat['task_id']\n",
    "    print(d[cif])\n",
    "    #break\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fout = open('mpcomplete_kitchaev_mpids.json', 'w')\n",
    "json.dump(d, fout)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mpworks]",
   "language": "python",
   "name": "conda-env-mpworks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
